 - Paper title: Global Encoding for Abstractive Summarization
 - Author: Junyang Lin and Xu Sun and Shuming Ma and Qi Su
 - From: ACL 2018



# Global Encoding for Abstractive Summarization

论文代码github地址：https://github.com/lancopku/Global-Encoding


# 论文总结
 - **针对的问题**：传统的序列到序列(seq2seq)模型生成的摘要经常会存在重复或者无语义的问题。
- **解决方案**：提出了基于源文本上下文的全局信息的全局编码框架，它负责控制编码器到解码器的信息流。
- **模型的实现**：它由一个卷积门控单元组成，用于执行全局编码以改进源端信息的表示。
- **模型的评价**：分别使用了中文数据集LCSTS和英文数据集English Gigaword进行训练，得到的Rouge值均比基线模型更好，性能更佳，重复的问题也减少了。
**亮点**：在seq2seq的encoder和decoder之间加入了一个门控卷积单元，这个门控卷积单元包含了一个类似inception的CNN结构和self-attention机制，它很好的考虑了文章的全局信息，保证输出的摘要具有通顺的语义，并解决了前面attention机制可能导致的词语重复的问题。
# 1.Introduction
生成式文本摘要问题可以看作是一个从序列到序列的映射问题，因此，具有编码器和解码器结构的seq2seq模型能很好的运用在文本摘要问题中。而注意力机制则是解码器根据原文本得到的注意力权重来按照不同的优先级获取编码器中的信息，在前人的众多试验下，证明了注意力机制会优于传统的方法。

attention机制中的问题，Zhou et al.(2017)提出，带有attention的seq2seq模型得到的文本摘要与原文本并没有很明显的对应关系，而且编码器的输出会包含attention的噪声。这导致Attention-based seq2seq模型生成的摘要会出现字词重复，语义不对应，语法错误，不能反映原文本的主要内容等问题。
例如下面这个例子中，seq2seq生成的摘要中出现了两次officially，出现这种情况的原因就是attention机制，第一个officially出现后，由于officially的attention分数还是很高，所以模型选择使用两次officially，这就导致了字词重复的问题。
![在这里插入图片描述](..\img\4.7.4.png)

为了解决这个问题，本文提出了一个生成式摘要的全局编码模型，该模型设置了一个卷积门控单元来对源文本上下文进行全局编码。这个基于CNN实现的门会过滤每个通过参数共享实现的基于全局文本的编码器的输出，这样，每个时间步骤的表示都会根据全局上下文进行细化。本文使用了**LCSTS**和**Gigaword**两个数据集对模型进行训练，得到的Rouge值都比基线模型要好，而且摘要中字词重复的问题有很大的改善。

# 2.Global Encoding
本文使用的模型是seq2seq模型，在编码器中设置了一个门控卷积单元来进行全局编码。它基于RNN编码器的输入，全局编码使用了CNN来提升词和全文之间的联系。

## 2.1 Attention-based seq2seq
Seq2seq模型就是两个RNN模型，一个作为编码器，一个作为解码器。
RNN编码器顺序的从词向量中获取每个词的信息，然后编码器最后一层隐藏层的隐藏状态以及源文本作为解码器的输入，也就是作为初始的隐藏状态。
编码器是一个**双向LSTM**，它能包含上下文两个方向的信息。
解码器是一个**单向LSTM**，它读取输入的词并逐字生成摘要，并将目标的词汇表映射到高维空间中。
在每个时间步骤中，解码器从目标词汇表中生成一个摘要单词，直到取样到句子的结束标签。

Attention：注意力机制，这里略述，需要详细解释请自己上网搜索
解码器的隐藏状态$s_t$和编码器的输出$h_i$的编码过程使用一个加权矩阵$W_a$来计算，然后得到一个全局的Attention $\alpha_{t,i}$和上下文信息$C_t$。

## 2.2 Convolutional Gated Unit （本文亮点）
**卷积门控单元**由一个类似**inception**结构的CNN和**self-attention**机制组成。

**CNN**：CNN的结构如下图，这个结构的加入主要是考虑了n-gram，即语言内部的关系，也就是短语。
这是一个一维的卷积单元，它提取了句子中的n-gram信息，它的作用应该是**提取句子中的n-gram信息，来保证生成的摘要句子具有通顺的语义**。
tips:这里使用两个核心为3的kernel是根据inception的设计原则，因为k=5的结构过于复杂，会增大计算量，所以替换成两个k=5的结构。
![在这里插入图片描述](..\img\4.7.5.png)
**Self-attention**：为了让模型能进一步关注整个文章的全局语义，我们使用self-attention机制来进一步加强全局信息，并且让模型能在每个time step都挖掘当前词语与文章中其他词语的关系。

**the gated unit**：这个门单元是由上述两个机制组合而成，因此它能找出数据中的n-gram信息和全局相关性信息。
在每个时间步骤，先调用这个CNN结构，使用$ReLU$这个非线性激活函数，生成一个新的输出结果，然后再调用self-attention机制，来挖掘全局相关性，将他们的输出打包成一个矩阵，然后将它设置为一个门。

最后，CNN考虑了n-gram信息，self-attention考虑了文章long-term的依赖关系，最后的输出值再调用sigmoid函数后在0到1之间，0表示门趋向于移除这些信息，1表示门趋向于保留这些信息。


## 3.模型最终结果
在lcsts数据集上的结果
![Lcsts](..\img\4.7.6.png)
在Gigaword数据集上的结果
![在这里插入图片描述](..\img\4.7.7.png)
## 4.结论
本文提出的使用门控卷积单元来进行源文本的全局编码，来保证核心信息能被保留，并且能过滤不重要的信息。本文根据两种数据集的实验后发现，该模型相比基线模型能很好的减少词语无意义的重复，并且对于不同长度的文本信息有很好的鲁棒性。


<div style="text-align: right"> by wu </div>
<div style="text-align: right"> 2019.02.27 </div>