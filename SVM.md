<!-- TOC -->

- [SVM](#svm)
  - [SVM的基本作用和实现原理](#svm%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%9C%E7%94%A8%E5%92%8C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86)
  - [SVM核函数的选择](#svm%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9)
- [L2正则化(岭回归)](#l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%B2%AD%E5%9B%9E%E5%BD%92)
- [L1正则化](#l1%E6%AD%A3%E5%88%99%E5%8C%96)
- [Dropout](#dropout)
- [LR和SVM的区别](#lr%E5%92%8Csvm%E7%9A%84%E5%8C%BA%E5%88%AB)
- [决策树算法](#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95)
  - [ID3算法](#id3%E7%AE%97%E6%B3%95)
  - [C4,5](#c45)
  - [CART](#cart)
- [随机森林](#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97)
- [Map端shuffle](#map%E7%AB%AFshuffle)
- [PCA降维](#pca%E9%99%8D%E7%BB%B4)
- [K-means](#k-means)

<!-- /TOC -->
# SVM
## SVM的基本作用和实现原理
SVM可以用于解决二分类或者多分类问题，此处以二分类为例。SVM的目标是寻找一个最优化超平面在空间中分割两类数据，这个最优化超平面需要满足的条件是：离其最近的点到其的距离最大化，这些点被称为支持向量。

## SVM核函数的选择
- 当样本的特征很多且维数很高时可考虑用SVM的线性核函数。
- 当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。
- 当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。

# L2正则化(岭回归)
直接在原来的损失函数基础上加上权重参数的平方和。

# L1正则化
直接在原来的损失函数基础上加上权重参数的绝对值。

L1正则化的解具有稀疏性，可用于特征选择。

L2正则化的解都比较小，抗扰动能力强。

# Dropout
Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，

# LR和SVM的区别
1）LR是参数模型，SVM是非参数模型。

2）从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。

3）SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

4）逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

5）logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

# 决策树算法
决策树思想，实际上就是寻找最纯净的划分方法，ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率作为不纯度；CART算法使用基尼系数作为不纯度。
## ID3算法
根据**最大信息增益**的原则作为划分当前数据集的最好特征。

## C4,5
通过**信息增益比**来作为划分当前数据集的特征。

## CART 
使用**基尼指数**来选择最好的数据划分。


ID3的缺点，倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树；另外输入变量必须是分类变量（连续变量必须离散化）；最后无法处理空值。

C4.5选择了信息增益率替代信息增益。

CART以基尼系数替代熵；最小化不纯度而不是最大化信息增益。

# 随机森林
1.  样本的随机：从样本集中用Bootstrap随机选取n个样本

2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）

3.  重复以上两步m次，即建立了m棵CART决策树

4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）

# Map端shuffle
1. 分区partition
2. 写入环形内存缓冲区Spill
3. 执行溢出写
    排序sort--->合并combiner--->生成溢出写文件
4. 归并merge
   1. merge是将所有的溢写文件归并到一个文件，结合上面所描述的combiner的作用范围，归并得到的文件内键值对有可能拥有相同的key，这个过程如果client设置过

Reduce端shuffle
1. 复制copy,使用HTTP的方式拉取数据
2. sort
3. 归并merge
   1. Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。这个最终文件可能在磁盘中也可能在内存中。当然我们希望它在内存中，直接作为reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当reducer的输入文件已定，整个shuffle才最终结束。然后就是reducer执行，把结果存放到HDFS上。
4. reduce

# PCA降维
通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。

1，对所有样本进行中心化，（对每个维度减去这个维度的数据均值）

2.计算样本的协方差矩阵

3.对协方差矩阵做特征值分解

4.选取前n个最大的特征值对应的的特征向量构成特征向量矩阵

# K-means
从数据集中随机选择k个聚类样本作为初始的聚类中心,然后计算数据集中每个样本到这k个聚类中心的距离,并将此样本分到距离最小的聚类中心所对应的类中。将所有样本归类后,对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心,重复以上操作直到聚类中心不变为止。

1）k-means是局部最优的，容易受到初始质心的影响；比如在下图中，因选择初始质心不恰当而造成次优的聚类结果。

2）同时，k值的选取也会直接影响聚类结果，最优聚类的k值应与样本数据本身的结构信息相吻合，而这种结构信息是很难去掌握，因此选取最优k值是非常困难的。