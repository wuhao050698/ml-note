Q:word2vec的计算流程：
> A:1. 首先是对语料进行预处理，比如中文语料的分词；去除语料中的停用词等等。
> 2. 扫描语料库，统计每个词的词频，然后保存到一个hash表里。
> 3. 根据词频建立Huffman树，Huffman树是一颗最优二叉树。
> 4. 根据哈夫曼树生成哈夫曼编码，每个叶子节点即词向量都对应一个编码。
> 5. 初始化非叶子节点的中间向量和叶节点中的词向量。
tips:在训练开始之前，词向量就已经存在，只不过是一个无意义的初始值，通过不断的迭代得到最优的词向量。
> 6. 训练中间向量和词向量

Q：word2vec的两种形式
> A:CBOW和Skip-Gram,CBOW是通过当前词的上下文来预测当前词，skip-gram是通过当前词来预测上下文的词。

Q：word2vec的两种算法
> A:层次softmax和负样本采样，层次softmax是通过Huffman树来实现的，它的中间节点相当于神经网络的隐藏层，叶子节点相当于输出，也就是每个词的词向量，负样本采样则是通过采样负样本，然后使用二元逻辑回归得到的。

Q：负样本采样的采样方式
> A:先假设一条长度为1的线段，然后根据词频将线段的比例将线段分给每个词，word2vec使用的是查表的方式，将这条线段标上M个刻度，M远大于词的数量，然后在0到M中取出n个点，就能得到n个负样本了。word2vec对词频的比例取了3/4，是一种平滑策略，让低频词的出场机会更多。

Q: SGD
> A:随机梯度下降算法就是对每个样本都迭代一次，更新一次梯度，因此它一般不需要使用全部的数据就已经找到了最优解，它的好处就是梯度的计算速度很快，训练速度也很快，它的缺点是会陷入局部最优解，而且损失函数的波动很大，噪声也比较多。

Q：Momentum
> A:引入了动量，也就是之前的历史梯度的累积。

Q：Adagard
> A:自适应学习率，学习率与历史梯度总和的平方根成反比。

Q: Adam
> A:利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,确定一个学习率。

Q: RNN
> A:循环神经网络，每一层神经网络有两个输入，包括一个正常的输入层和前一个神经元的隐藏层的输出，然后它的输出也有两个，包括一个隐藏层的输出给下一个神经元，和一个输出层的输出。

Q: RNN的好处
> A:传统的神经网络都是一个一个输入的，前一个输入和后一个输入之间没有任何关联，为了更好的获取序列的信息，我们使用RNN模型。

Q：RNN存在的问题
> A:无法解决长期依赖的问题，当两个输入的信息之间距离过长时，RNN无法很好的学习到它们之间的信息。

Q: LSTM
> A: long term short term memory networks，长短记忆神经网络，顾名思义，它就是为了解决长期依赖问题设计出来的模型，