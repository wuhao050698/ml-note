Q:word2vec的计算流程：
> A:1. 首先是对语料进行预处理，比如中文语料的分词；去除语料中的停用词等等。
> 2. 扫描语料库，统计每个词的词频，然后保存到一个hash表里。
> 3. 根据词频建立Huffman树，Huffman树是一颗最优二叉树。
> 4. 根据哈夫曼树生成哈夫曼编码，每个叶子节点即词向量都对应一个编码。
> 5. 初始化非叶子节点的中间向量和叶节点中的词向量。
tips:在训练开始之前，词向量就已经存在，只不过是一个无意义的初始值，通过不断的迭代得到最优的词向量。
> 6. 训练中间向量和词向量

Q：word2vec的两种形式
> A:CBOW和Skip-Gram,CBOW是通过当前词的上下文来预测当前词，skip-gram是通过当前词来预测上下文的词。

Q：word2vec的两种算法
> A:层次softmax和负样本采样，层次softmax是通过Huffman树来实现的，它的中间节点相当于神经网络的隐藏层，叶子节点相当于输出，也就是每个词的词向量，负样本采样则是通过采样负样本，然后使用二元逻辑回归得到的。

Q：负样本采样的采样方式
> A:先假设一条长度为1的线段，然后根据词频将线段的比例将线段分给每个词，word2vec使用的是查表的方式，将这条线段标上M个刻度，M远大于词的数量，然后在0到M中取出n个点，就能得到n个负样本了。word2vec对词频的比例取了3/4，是一种平滑策略，让低频词的出场机会更多。

Q: SGD
> A:随机梯度下降算法就是对每个样本都迭代一次，更新一次梯度，因此它一般不需要使用全部的数据就已经找到了最优解，它的好处就是梯度的计算速度很快，训练速度也很快，它的缺点是会陷入局部最优解，而且损失函数的波动很大，噪声也比较多。

Q：Momentum
> A:引入了动量，也就是之前的历史梯度的累积。

Q：Adagard
> A:自适应学习率，学习率与历史梯度总和的平方根成反比。

Q: Adam
> A:利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,确定一个学习率。

Q: RNN
> A:循环神经网络，每一层神经网络有两个输入，包括一个正常的输入层和前一个神经元的隐藏层的输出，它将两个输入合并成一个变量后，使用tanh函数进行激活，然后它的输出也有两个，包括一个隐藏层的输出给下一个神经元，和一个输出层的输出。

Q: RNN的好处
> A:传统的神经网络都是一个一个输入的，前一个输入和后一个输入之间没有任何关联，为了更好的获取序列的信息，我们使用RNN模型。

Q：RNN存在的问题
> A:无法解决长期依赖的问题，当两个输入的信息之间距离过长时，RNN无法很好的学习到它们之间的信息。

Q: LSTM
> A: long term short term memory networks，长短记忆神经网络，顾名思义，它就是为了解决长期依赖问题设计出来的模型。
> 
> LSTM中包含了三种门，遗忘门，输入门，输出门。
>
>遗忘门：来自前一个隐藏层的输入和当前输入信息合并后输入到sigmoid函数，映射到(0,1)之间，越接近0越容易忘记。
>
>输入门：同样将隐藏状态和当前输入分别输入到sigmoid函数和tanh函数中，然后将这两个函数得到的输出相乘，sigmoid函数会决定tanh中哪些重要的信息需要保留。
>
>计算单元状态(细胞状态)，先把之前的单元状态与遗忘门输出的遗忘向量相乘，然后再加上输入门输出的向量，就得到了新的单元状态。
>
>输出门：输出门决定了下一个隐藏状态的值，首先把输入的隐藏状态和当前输入传递给sigmoid函数来确定应该保存的信息，然后将当前计算后的细胞状态输入到tanh中，将这两个输出相乘，就得到了下一个隐藏状态的值，然后输出到下一个神经元中。

Q：如何解释梯度爆炸和梯度消失
> A:例如一个tanh函数，它的y值位于(0,1)之间，在靠近1或者-1时导数接近于0，梯度消失，在靠近0时，导数无限大，梯度爆炸，会剧烈的波动。

Q：GRU
> A:GRU是LSTM的变体，它只有两个门，一个是更新门，另一个是重置门。它并没有细胞状态，只传递隐藏状态。首先是重置门，它是上一个隐藏状态和当前输入合并后输入到sigmoid函数中，然后乘上上一个隐藏状态的值，然后和当前输入合并后经过tanh，然后更新门是隐藏状态和当前输入输入到sigmoid，然后1-得到的结果，和隐藏状态相乘，更新隐藏状态，然后用刚刚重置门得到的结果和更新门sigmoid得到的结果相乘，加到新的隐藏状态中，输出结果。

Q:dropout
> A:dropout是指在神经网络的训练过程中，按一定的概率随机丢弃一部分隐藏层神经元，这样可以有效的接近神经网络中普遍出现的过拟合的问题。

<div style="text-align: right"> by wu </div>
<div style="text-align: right"> 2019.04.06 </div>