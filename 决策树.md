# 决策树算法
决策树思想，实际上就是寻找最纯净的划分方法，ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率作为不纯度；CART算法使用基尼系数作为不纯度。
## ID3算法
根据**最大信息增益**的原则作为划分当前数据集的最好特征。

## C4,5
通过**信息增益比**来作为划分当前数据集的特征。

## CART 
使用**基尼指数**来选择最好的数据划分。


ID3的缺点，倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树；另外输入变量必须是分类变量（连续变量必须离散化）；最后无法处理空值。

C4.5选择了信息增益率替代信息增益。

CART以基尼系数替代熵；最小化不纯度而不是最大化信息增益。

# 随机森林
1.  样本的随机：从样本集中用Bootstrap随机选取n个样本

2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）

3.  重复以上两步m次，即建立了m棵CART决策树

4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）

